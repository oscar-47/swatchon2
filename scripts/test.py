import os
import sys
import json
import argparse
import time
from typing import List, Set

from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError

# The base filtered URL provided by the user (woven subcategories)
BASE_FILTERED = (
    "https://swatchon.com/wholesale-fabric?"
    "categoryIds=167,181,178,179,182,169,176,168,193,192,172,258,187,234,173,170&sort=&from=/wholesale-fabric"
)


def build_page_url(page: int) -> str:
    """æ„å»ºé¡µé¢URL"""
    # ç¬¬ä¸€é¡µå’Œå…¶ä»–é¡µé¢éƒ½ä½¿ç”¨ç»Ÿä¸€çš„æ ¼å¼
    return (
        "https://swatchon.com/wholesale-fabric?"
        f"categoryIds=167,181,178,179,182,169,176,168,193,192,172,258,187,234,173,170&sort=&page={page}&from=/wholesale-fabric"
    )


def accept_cookies(page) -> None:
    """æ¥å—cookies"""
    try:
        print("[debug] å°è¯•æ¥å— cookies...", flush=True)
        cookie_selectors = [
            "button:has-text('Accept')",
            "button:has-text('I agree')",
            "button:has-text('åŒæ„')",
            "text=Accept",
            "[id*='cookie'] button",
            "[class*='cookie'] button",
            "button[class*='accept']",
            ".cookie-banner button",
            "#cookie-consent button"
        ]
        
        for sel in cookie_selectors:
            try:
                btn = page.locator(sel).first
                if btn and btn.count() > 0 and btn.is_visible():
                    print(f"[debug] æ‰¾åˆ°å¹¶ç‚¹å‡» cookie æŒ‰é’®: {sel}", flush=True)
                    btn.click()
                    page.wait_for_timeout(1000)
                    return
            except Exception as e:
                continue
        print("[debug] æœªæ‰¾åˆ° cookie æŒ‰é’®", flush=True)
    except Exception as e:
        print(f"[debug] Cookie å¤„ç†å‡ºé”™: {e}", flush=True)


def collect_page_links(page, page_num: int) -> List[str]:
    """ä»å•ä¸ªé¡µé¢æ”¶é›†é“¾æ¥"""
    print(f"[debug] å¼€å§‹æ”¶é›†ç¬¬ {page_num} é¡µé“¾æ¥...", flush=True)
    
    try:
        # æŸ¥æ‰¾æœç´¢å®¹å™¨
        container = page.locator("div.search-items").first
        if container.count() == 0:
            print(f"[error] ç¬¬ {page_num} é¡µæœªæ‰¾åˆ°æœç´¢ç»“æœå®¹å™¨", flush=True)
            return []
        
        container.wait_for(state="visible", timeout=10000)
        
        # æŸ¥æ‰¾äº§å“å¡ç‰‡
        cards = container.locator(".c-quality")
        total = cards.count()
        print(f"[debug] ç¬¬ {page_num} é¡µæ‰¾åˆ° {total} ä¸ªäº§å“å¡ç‰‡", flush=True)
        
        if total == 0:
            print(f"[warning] ç¬¬ {page_num} é¡µæ²¡æœ‰æ‰¾åˆ°äº§å“å¡ç‰‡", flush=True)
            return []
        
        page_links: List[str] = []
        
        # æå–æ¯ä¸ªå¡ç‰‡çš„é“¾æ¥ - ç›´æ¥ä»divçš„hrefå±æ€§è·å–
        for i in range(total):
            try:
                c = cards.nth(i)
                
                # ç›´æ¥ä».c-quality divè·å–hrefå±æ€§
                href = c.get_attribute("href", timeout=2000)
                
                if href:
                    # è§„èŒƒåŒ–URL
                    if href.startswith("/"):
                        href = "https://swatchon.com" + href
                    elif not href.startswith("http"):
                        href = "https://swatchon.com/" + href
                    
                    page_links.append(href)
                    print(f"[debug] ç¬¬ {page_num} é¡µå¡ç‰‡ {i+1} æ‰¾åˆ°é“¾æ¥: {href}", flush=True)
                else:
                    print(f"[debug] ç¬¬ {page_num} é¡µå¡ç‰‡ {i+1} æ²¡æœ‰hrefå±æ€§", flush=True)
                    
            except Exception as e:
                print(f"[debug] ç¬¬ {page_num} é¡µå¡ç‰‡ {i+1} å¤„ç†å¤±è´¥: {e}", flush=True)
                continue
        
        print(f"[success] ç¬¬ {page_num} é¡µæˆåŠŸæå– {len(page_links)} ä¸ªé“¾æ¥", flush=True)
        return page_links
        
    except Exception as e:
        print(f"[error] ç¬¬ {page_num} é¡µé“¾æ¥æ”¶é›†å¤±è´¥: {e}", flush=True)
        return []


def scrape_multiple_pages(target_count: int = 150, start_page: int = 1, max_pages: int = 20) -> dict:
    """çˆ¬å–å¤šä¸ªé¡µé¢ç›´åˆ°è¾¾åˆ°ç›®æ ‡é“¾æ¥æ•°é‡"""
    
    all_links: Set[str] = set()  # ä½¿ç”¨setè‡ªåŠ¨å»é‡
    page_results = []
    
    with sync_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨
        browser = p.chromium.launch(
            headless=True,
            args=[
                "--no-sandbox",
                "--disable-blink-features=AutomationControlled",
                "--disable-dev-shm-usage",
            ]
        )
        
        context = browser.new_context(
            user_agent=(
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                "(KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36"
            ),
            locale="en-US",
            viewport={"width": 1920, "height": 1080},
        )
        
        page = context.new_page()
        page.set_default_timeout(30000)
        
        print(f"[info] å¼€å§‹å¤šé¡µé¢çˆ¬å–ï¼Œç›®æ ‡: {target_count} ä¸ªé“¾æ¥", flush=True)
        print("=" * 60)
        
        current_page = start_page
        cookies_accepted = False
        
        while len(all_links) < target_count and current_page <= max_pages:
            try:
                url = build_page_url(current_page)
                print(f"\n[step] æ­£åœ¨çˆ¬å–ç¬¬ {current_page} é¡µ", flush=True)
                print(f"[url] {url}", flush=True)
                
                # è®¿é—®é¡µé¢
                response = page.goto(url, wait_until="domcontentloaded", timeout=30000)
                if not response or response.status != 200:
                    print(f"[error] ç¬¬ {current_page} é¡µè®¿é—®å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status if response else 'None'}", flush=True)
                    current_page += 1
                    continue
                
                # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
                print(f"[debug] ç¬¬ {current_page} é¡µç­‰å¾…ç½‘ç»œç©ºé—²...", flush=True)
                try:
                    page.wait_for_load_state("networkidle", timeout=15000)
                except PlaywrightTimeoutError:
                    print(f"[debug] ç¬¬ {current_page} é¡µç½‘ç»œç©ºé—²ç­‰å¾…è¶…æ—¶", flush=True)
                
                # ç­‰å¾…APIæ•°æ®åŠ è½½ï¼ˆäº§å“åˆ—è¡¨é€šå¸¸é€šè¿‡APIè·å–ï¼‰
                print(f"[debug] ç¬¬ {current_page} é¡µç­‰å¾…äº§å“æ•°æ®åŠ è½½...", flush=True)
                try:
                    # ç­‰å¾…äº§å“å¡ç‰‡å‡ºç°
                    page.wait_for_selector(".c-quality", timeout=10000)
                    # å†ç­‰å¾…ä¸€ä¸‹ç¡®ä¿æ‰€æœ‰å¡ç‰‡éƒ½åŠ è½½å®Œæˆ
                    time.sleep(2)
                except PlaywrightTimeoutError:
                    print(f"[warning] ç¬¬ {current_page} é¡µäº§å“å¡ç‰‡åŠ è½½è¶…æ—¶", flush=True)
                
                # ç¬¬ä¸€æ¬¡è®¿é—®æ—¶æ¥å—cookies
                if not cookies_accepted:
                    accept_cookies(page)
                    cookies_accepted = True
                
                # æ”¶é›†å½“å‰é¡µé¢çš„é“¾æ¥
                page_links = collect_page_links(page, current_page)
                
                if not page_links:
                    print(f"[warning] ç¬¬ {current_page} é¡µæ²¡æœ‰æ‰¾åˆ°é“¾æ¥ï¼Œå¯èƒ½å·²åˆ°æœ€åä¸€é¡µ", flush=True)
                    break
                
                # æ·»åŠ åˆ°æ€»é“¾æ¥é›†åˆä¸­ï¼ˆè‡ªåŠ¨å»é‡ï¼‰
                before_count = len(all_links)
                all_links.update(page_links)
                new_links_count = len(all_links) - before_count
                
                # è®°å½•é¡µé¢ç»“æœ
                page_result = {
                    "page": current_page,
                    "url": url,
                    "links_found": len(page_links),
                    "new_unique_links": new_links_count,
                    "total_unique_links": len(all_links)
                }
                page_results.append(page_result)
                
                print(f"[progress] ç¬¬ {current_page} é¡µ: æ‰¾åˆ° {len(page_links)} ä¸ªé“¾æ¥, æ–°å¢ {new_links_count} ä¸ªå”¯ä¸€é“¾æ¥", flush=True)
                print(f"[progress] æ€»è¿›åº¦: {len(all_links)}/{target_count} ä¸ªå”¯ä¸€é“¾æ¥", flush=True)
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
                if len(all_links) >= target_count:
                    print(f"\n[success] ğŸ‰ å·²è¾¾åˆ°ç›®æ ‡ï¼å…±æ”¶é›† {len(all_links)} ä¸ªå”¯ä¸€é“¾æ¥", flush=True)
                    break
                
                current_page += 1
                
                # é¡µé¢é—´ç¨ä½œå»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(1)
                
            except Exception as e:
                print(f"[error] ç¬¬ {current_page} é¡µå¤„ç†å‡ºé”™: {e}", flush=True)
                current_page += 1
                continue
        
        context.close()
        browser.close()
        
        # è¿”å›ç»“æœ
        result = {
            "timestamp": time.time(),
            "target_count": target_count,
            "actual_count": len(all_links),
            "pages_scraped": len(page_results),
            "page_details": page_results,
            "all_links": sorted(list(all_links))  # è½¬æ¢ä¸ºæ’åºçš„åˆ—è¡¨
        }
        
        return result


def main():
    parser = argparse.ArgumentParser(description="å¤šé¡µé¢çˆ¬å–äº§å“é“¾æ¥ï¼Œè¾¾åˆ°æŒ‡å®šæ•°é‡ååœæ­¢")
    parser.add_argument("--target", type=int, default=150, help="ç›®æ ‡é“¾æ¥æ•°é‡ (default: 150)")
    parser.add_argument("--start-page", type=int, default=1, help="èµ·å§‹é¡µé¢ (default: 1)")
    parser.add_argument("--max-pages", type=int, default=20, help="æœ€å¤§é¡µé¢æ•°é™åˆ¶ (default: 20)")
    parser.add_argument("--out", type=str, default=None, help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    args = parser.parse_args()
    
    print(f"[info] å¤šé¡µé¢é“¾æ¥æ”¶é›†å™¨å¯åŠ¨", flush=True)
    print(f"[info] ç›®æ ‡é“¾æ¥æ•°é‡: {args.target}", flush=True)
    print(f"[info] èµ·å§‹é¡µé¢: {args.start_page}", flush=True)
    print(f"[info] æœ€å¤§é¡µé¢é™åˆ¶: {args.max_pages}", flush=True)
    
    try:
        # å¼€å§‹çˆ¬å–
        result = scrape_multiple_pages(
            target_count=args.target,
            start_page=args.start_page,
            max_pages=args.max_pages
        )
        
        # ä¿å­˜ç»“æœ
        out_dir = os.path.join(os.getcwd(), "outputs")
        os.makedirs(out_dir, exist_ok=True)
        
        if args.out:
            out_path = args.out
        else:
            timestamp_str = time.strftime("%Y%m%d_%H%M%S")
            out_path = os.path.join(out_dir, f"swatchon_links_{args.target}_{timestamp_str}.json")
        
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"\n[saved] ç»“æœå·²ä¿å­˜åˆ°: {out_path}", flush=True)
        
        # è¾“å‡ºæ±‡æ€»ä¿¡æ¯
        print("\n" + "=" * 60)
        print("ğŸ“Š çˆ¬å–æ±‡æ€»:")
        print(f"   ç›®æ ‡æ•°é‡: {result['target_count']}")
        print(f"   å®é™…æ”¶é›†: {result['actual_count']}")
        print(f"   çˆ¬å–é¡µé¢: {result['pages_scraped']}")
        print(f"   å®Œæˆç‡: {result['actual_count']/result['target_count']*100:.1f}%")
        
        print(f"\nğŸ“„ é¡µé¢è¯¦æƒ…:")
        for page_detail in result['page_details']:
            print(f"   ç¬¬ {page_detail['page']} é¡µ: {page_detail['links_found']} ä¸ªé“¾æ¥ (æ–°å¢ {page_detail['new_unique_links']} ä¸ª)")
        
        print(f"\nğŸ”— å‰10ä¸ªé“¾æ¥ç¤ºä¾‹:")
        for i, link in enumerate(result['all_links'][:10], 1):
            print(f"   {i:2d}. {link}")
        if len(result['all_links']) > 10:
            print(f"   ... è¿˜æœ‰ {len(result['all_links']) - 10} ä¸ªé“¾æ¥")
            
    except KeyboardInterrupt:
        print(f"\n[info] ç”¨æˆ·ä¸­æ–­çˆ¬å–", flush=True)
    except Exception as e:
        print(f"\n[error] çˆ¬å–è¿‡ç¨‹å‡ºé”™: {e}", flush=True)
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()