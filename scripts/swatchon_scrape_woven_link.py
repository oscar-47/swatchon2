import os
import sys
import json
import time
from typing import List, Set, Dict

from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError


# å®šä¹‰æ‰€æœ‰ç±»åˆ«çš„é…ç½®
CATEGORIES = {
    "Plain": {
        "categoryIds": "167,181,178,179,182,169,176,168,193,192,172,258,187,234,173,170",
        "url": "https://swatchon.com/wholesale-fabric?categoryIds=167,181,178,179,182,169,176,168,193,192,172,258,187,234,173,170&sort=&from=/wholesale-fabric"
    },
    "Twill_Weave": {
        "categoryIds": "186,189,175,253,196,194",
        "url": "https://swatchon.com/wholesale-fabric?categoryIds=186,189,175,253,196,194&sort=&from=/wholesale-fabric"
    },
    "Satin_Weave": {
        "categoryIds": "254,255,256",
        "url": "https://swatchon.com/wholesale-fabric?categoryIds=254,255,256&sort=&from=/wholesale-fabric"
    },
    "Jacquard_Weave": {
        "categoryIds": "184,183",
        "url": "https://swatchon.com/wholesale-fabric?categoryIds=184,183&sort=&from=/wholesale-fabric"
    },
    "Pile_Weave": {
        "categoryIds": "188,247",
        "url": "https://swatchon.com/wholesale-fabric?categoryIds=188,247&sort=&from=/wholesale-fabric"
    },
    "Dobby": {
        "categoryIds": "171",
        "url": "https://swatchon.com/wholesale-fabric?categoryIds=171&sort=&from=/wholesale-fabric"
    },
    "Double_Weave": {
        "categoryIds": "185",
        "url": "https://swatchon.com/wholesale-fabric?categoryIds=185&sort=&from=/wholesale-fabric"
    },
    "Eyelet": {
        "categoryIds": "177",
        "url": "https://swatchon.com/wholesale-fabric?categoryIds=177&sort=&from=/wholesale-fabric"
    },
    "Ripstop": {
        "categoryIds": "191",
        "url": "https://swatchon.com/wholesale-fabric?categoryIds=191&sort=&from=/wholesale-fabric"
    }
}


def build_category_page_url(category_ids: str, page: int) -> str:
    """æ„å»ºåˆ†ç±»é¡µé¢URL"""
    return f"https://swatchon.com/wholesale-fabric?categoryIds={category_ids}&sort=&page={page}&from=/wholesale-fabric"


def accept_cookies(page) -> None:
    """æ¥å—cookies"""
    try:
        print("[debug] å°è¯•æ¥å— cookies...", flush=True)
        cookie_selectors = [
            "button:has-text('Accept')",
            "button:has-text('I agree')",
            "button:has-text('åŒæ„')",
            "text=Accept",
        ]
        
        for sel in cookie_selectors:
            try:
                btn = page.locator(sel).first
                if btn and btn.count() > 0 and btn.is_visible():
                    print(f"[debug] æ‰¾åˆ°å¹¶ç‚¹å‡» cookie æŒ‰é’®", flush=True)
                    btn.click()
                    page.wait_for_timeout(1000)
                    return
            except Exception:
                continue
        print("[debug] æœªæ‰¾åˆ° cookie æŒ‰é’®", flush=True)
    except Exception as e:
        print(f"[debug] Cookie å¤„ç†å‡ºé”™: {e}", flush=True)


def collect_page_links(page, page_num: int, category_name: str) -> List[str]:
    """ä»å•ä¸ªé¡µé¢æ”¶é›†é“¾æ¥"""
    print(f"[debug] {category_name} - å¼€å§‹æ”¶é›†ç¬¬ {page_num} é¡µé“¾æ¥...", flush=True)
    
    try:
        # æŸ¥æ‰¾æœç´¢å®¹å™¨
        container = page.locator("div.search-items").first
        if container.count() == 0:
            print(f"[error] {category_name} - ç¬¬ {page_num} é¡µæœªæ‰¾åˆ°æœç´¢ç»“æœå®¹å™¨", flush=True)
            return []
        
        container.wait_for(state="visible", timeout=10000)
        
        # æŸ¥æ‰¾äº§å“å¡ç‰‡
        cards = container.locator(".c-quality")
        total = cards.count()
        print(f"[debug] {category_name} - ç¬¬ {page_num} é¡µæ‰¾åˆ° {total} ä¸ªäº§å“å¡ç‰‡", flush=True)
        
        if total == 0:
            print(f"[warning] {category_name} - ç¬¬ {page_num} é¡µæ²¡æœ‰æ‰¾åˆ°äº§å“å¡ç‰‡", flush=True)
            return []
        
        page_links: List[str] = []
        
        # æå–æ¯ä¸ªå¡ç‰‡çš„é“¾æ¥ - ç›´æ¥ä»divçš„hrefå±æ€§è·å–
        for i in range(total):
            try:
                c = cards.nth(i)
                href = c.get_attribute("href", timeout=2000)
                
                if href:
                    # è§„èŒƒåŒ–URL
                    if href.startswith("/"):
                        href = "https://swatchon.com" + href
                    elif not href.startswith("http"):
                        href = "https://swatchon.com/" + href
                    
                    page_links.append(href)
                    
            except Exception as e:
                print(f"[debug] {category_name} - ç¬¬ {page_num} é¡µå¡ç‰‡ {i+1} å¤„ç†å¤±è´¥: {e}", flush=True)
                continue
        
        print(f"[success] {category_name} - ç¬¬ {page_num} é¡µæˆåŠŸæå– {len(page_links)} ä¸ªé“¾æ¥", flush=True)
        return page_links
        
    except Exception as e:
        print(f"[error] {category_name} - ç¬¬ {page_num} é¡µé“¾æ¥æ”¶é›†å¤±è´¥: {e}", flush=True)
        return []


def scrape_category(category_name: str, category_config: dict, target_count: int = 150, max_pages: int = 20) -> dict:
    """çˆ¬å–å•ä¸ªåˆ†ç±»çš„æ‰€æœ‰é“¾æ¥"""
    
    print(f"\n{'='*80}")
    print(f"ğŸ¯ å¼€å§‹çˆ¬å–åˆ†ç±»: {category_name}")
    print(f"ğŸ“Š ç›®æ ‡é“¾æ¥æ•°é‡: {target_count}")
    print(f"ğŸ”— åˆ†ç±»URL: {category_config['url']}")
    print(f"{'='*80}")
    
    all_links: Set[str] = set()
    page_results = []
    
    with sync_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨
        browser = p.chromium.launch(
            headless=True,
            args=[
                "--no-sandbox",
                "--disable-blink-features=AutomationControlled",
                "--disable-dev-shm-usage",
            ]
        )
        
        context = browser.new_context(
            user_agent=(
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                "(KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36"
            ),
            locale="en-US",
            viewport={"width": 1920, "height": 1080},
        )
        
        page = context.new_page()
        page.set_default_timeout(30000)
        
        current_page = 1
        cookies_accepted = False
        
        while len(all_links) < target_count and current_page <= max_pages:
            try:
                url = build_category_page_url(category_config["categoryIds"], current_page)
                print(f"\n[step] {category_name} - æ­£åœ¨çˆ¬å–ç¬¬ {current_page} é¡µ")
                
                # è®¿é—®é¡µé¢
                response = page.goto(url, wait_until="domcontentloaded", timeout=30000)
                if not response or response.status != 200:
                    print(f"[error] {category_name} - ç¬¬ {current_page} é¡µè®¿é—®å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status if response else 'None'}")
                    current_page += 1
                    continue
                
                # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
                try:
                    page.wait_for_load_state("networkidle", timeout=15000)
                except PlaywrightTimeoutError:
                    print(f"[debug] {category_name} - ç¬¬ {current_page} é¡µç½‘ç»œç©ºé—²ç­‰å¾…è¶…æ—¶")
                
                # ç­‰å¾…äº§å“å¡ç‰‡åŠ è½½
                try:
                    page.wait_for_selector(".c-quality", timeout=10000)
                    time.sleep(2)  # é¢å¤–ç­‰å¾…ç¡®ä¿åŠ¨æ€å†…å®¹åŠ è½½å®Œæˆ
                except PlaywrightTimeoutError:
                    print(f"[warning] {category_name} - ç¬¬ {current_page} é¡µäº§å“å¡ç‰‡åŠ è½½è¶…æ—¶")
                
                # ç¬¬ä¸€æ¬¡è®¿é—®æ—¶æ¥å—cookies
                if not cookies_accepted:
                    accept_cookies(page)
                    cookies_accepted = True
                
                # æ”¶é›†å½“å‰é¡µé¢çš„é“¾æ¥
                page_links = collect_page_links(page, current_page, category_name)
                
                if not page_links:
                    print(f"[info] {category_name} - ç¬¬ {current_page} é¡µæ²¡æœ‰æ‰¾åˆ°é“¾æ¥ï¼Œå¯èƒ½å·²åˆ°æœ€åä¸€é¡µ")
                    break
                
                # æ·»åŠ åˆ°æ€»é“¾æ¥é›†åˆä¸­ï¼ˆè‡ªåŠ¨å»é‡ï¼‰
                before_count = len(all_links)
                all_links.update(page_links)
                new_links_count = len(all_links) - before_count
                
                # è®°å½•é¡µé¢ç»“æœ
                page_result = {
                    "page": current_page,
                    "url": url,
                    "links_found": len(page_links),
                    "new_unique_links": new_links_count,
                    "total_unique_links": len(all_links)
                }
                page_results.append(page_result)
                
                print(f"[progress] {category_name} - ç¬¬ {current_page} é¡µ: æ‰¾åˆ° {len(page_links)} ä¸ªé“¾æ¥, æ–°å¢ {new_links_count} ä¸ªå”¯ä¸€é“¾æ¥")
                print(f"[progress] {category_name} - æ€»è¿›åº¦: {len(all_links)}/{target_count} ä¸ªå”¯ä¸€é“¾æ¥")
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
                if len(all_links) >= target_count:
                    print(f"[success] {category_name} - ğŸ‰ å·²è¾¾åˆ°ç›®æ ‡ï¼å…±æ”¶é›† {len(all_links)} ä¸ªå”¯ä¸€é“¾æ¥")
                    break
                
                current_page += 1
                
                # é¡µé¢é—´ç¨ä½œå»¶è¿Ÿ
                time.sleep(1)
                
            except Exception as e:
                print(f"[error] {category_name} - ç¬¬ {current_page} é¡µå¤„ç†å‡ºé”™: {e}")
                current_page += 1
                continue
        
        context.close()
        browser.close()
        
        # è¿”å›ç»“æœ
        result = {
            "category": category_name,
            "timestamp": time.time(),
            "target_count": target_count,
            "actual_count": len(all_links),
            "pages_scraped": len(page_results),
            "page_details": page_results,
            "all_links": sorted(list(all_links))
        }
        
        return result


def main():
    """ä¸»å‡½æ•° - ä¾æ¬¡çˆ¬å–æ‰€æœ‰åˆ†ç±»"""
    
    print("ğŸš€ SwatchOn å…¨è‡ªåŠ¨åˆ†ç±»çˆ¬è™«å¯åŠ¨")
    print("=" * 80)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    base_output_dir = os.path.join(os.getcwd(), "outputs", "categories")
    os.makedirs(base_output_dir, exist_ok=True)
    
    # æ€»ä½“ç»Ÿè®¡
    total_start_time = time.time()
    all_results = {}
    overall_stats = {
        "total_categories": len(CATEGORIES),
        "completed_categories": 0,
        "total_links": 0,
        "start_time": total_start_time,
        "category_results": {}
    }
    
    # ä¾æ¬¡çˆ¬å–æ¯ä¸ªåˆ†ç±»
    for i, (category_name, category_config) in enumerate(CATEGORIES.items(), 1):
        try:
            print(f"\nğŸ—ï¸  å¤„ç†åˆ†ç±» {i}/{len(CATEGORIES)}: {category_name}")
            
            # çˆ¬å–åˆ†ç±»
            category_result = scrape_category(category_name, category_config, target_count=150)
            all_results[category_name] = category_result
            
            # ä¿å­˜åˆ†ç±»ç»“æœ
            category_output_dir = os.path.join(base_output_dir, category_name)
            os.makedirs(category_output_dir, exist_ok=True)
            
            timestamp_str = time.strftime("%Y%m%d_%H%M%S")
            category_file = os.path.join(category_output_dir, f"{category_name}_links_{timestamp_str}.json")
            
            with open(category_file, "w", encoding="utf-8") as f:
                json.dump(category_result, f, ensure_ascii=False, indent=2)
            
            # æ›´æ–°æ€»ä½“ç»Ÿè®¡
            overall_stats["completed_categories"] += 1
            overall_stats["total_links"] += category_result["actual_count"]
            overall_stats["category_results"][category_name] = {
                "links_count": category_result["actual_count"],
                "pages_scraped": category_result["pages_scraped"],
                "file_path": category_file
            }
            
            # è¾“å‡ºåˆ†ç±»å®Œæˆä¿¡æ¯
            print(f"\nâœ… {category_name} å®Œæˆ!")
            print(f"   ğŸ“ ä¿å­˜è·¯å¾„: {category_file}")
            print(f"   ğŸ“Š é“¾æ¥æ•°é‡: {category_result['actual_count']}")
            print(f"   ğŸ“„ çˆ¬å–é¡µé¢: {category_result['pages_scraped']}")
            
            # åˆ†ç±»é—´ä¼‘æ¯ä¸€ä¸‹
            if i < len(CATEGORIES):
                print(f"\nâ±ï¸  ä¼‘æ¯ 3 ç§’åç»§ç»­ä¸‹ä¸€ä¸ªåˆ†ç±»...")
                time.sleep(3)
                
        except Exception as e:
            print(f"âŒ {category_name} çˆ¬å–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # è®¡ç®—æ€»è€—æ—¶
    total_time = time.time() - total_start_time
    overall_stats["end_time"] = time.time()
    overall_stats["total_duration"] = total_time
    
    # ä¿å­˜æ€»ä½“æŠ¥å‘Š
    report_file = os.path.join(base_output_dir, f"overall_report_{time.strftime('%Y%m%d_%H%M%S')}.json")
    with open(report_file, "w", encoding="utf-8") as f:
        json.dump(overall_stats, f, ensure_ascii=False, indent=2)
    
    # è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š
    print(f"\n{'='*80}")
    print("ğŸŠ å…¨éƒ¨çˆ¬å–å®Œæˆï¼")
    print(f"{'='*80}")
    print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
    print(f"   â€¢ å¤„ç†åˆ†ç±»æ•°: {overall_stats['completed_categories']}/{overall_stats['total_categories']}")
    print(f"   â€¢ æ€»é“¾æ¥æ•°: {overall_stats['total_links']}")
    print(f"   â€¢ æ€»è€—æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")
    print(f"   â€¢ æŠ¥å‘Šæ–‡ä»¶: {report_file}")
    
    print(f"\nğŸ“‹ å„åˆ†ç±»è¯¦æƒ…:")
    for category_name, result in overall_stats["category_results"].items():
        print(f"   â€¢ {category_name:<15}: {result['links_count']:>3} é“¾æ¥ ({result['pages_scraped']} é¡µ)")
    
    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {base_output_dir}")
    print("ğŸ¯ æ‰€æœ‰åˆ†ç±»çš„é“¾æ¥å·²ä¿å­˜åˆ°å¯¹åº”çš„æ–‡ä»¶å¤¹ä¸­ï¼")


if __name__ == "__main__":
    main()